{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "class Vanilla_rnn:\n",
    "    \"\"\"\n",
    "    Vanilla recurrent neural network for language modelling at word level\n",
    "    Author: Manuel Plank @Manuel030\n",
    "    loss function calculation as in https://gist.github.com/karpathy/d4dee566867f8291f086\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, rate, units, seq_length, data):\n",
    "        \"\"\"define hyperparameters and data. Data should be string\"\"\"\n",
    "        self.learning_rate = rate\n",
    "        self.hidden_units = units\n",
    "        self.seq_length = seq_length\n",
    "        self.data = data\n",
    "        self.feature_vec_size = 0\n",
    "        \n",
    "    def __preprocess(self):\n",
    "        tokens = self.data.lower().split()\n",
    "        pattern = re.compile(r'\\w*\\.') # identifies sentences separated by points\n",
    "        data = [word[:-1] if bool(pattern.match(word)) else word for word in tokens] # words only\n",
    "\n",
    "        # one-hot encode words\n",
    "        vocabulary = list(set(data))\n",
    "        self.feature_vec_size = len(vocabulary)\n",
    "        voc_vec = pd.get_dummies(vocabulary).to_numpy()\n",
    "        dic = dict(zip(vocabulary, voc_vec))\n",
    "        # represent data as sequence of vectors\n",
    "        xs = []\n",
    "        for word in data:\n",
    "            xs.append(dic[word])\n",
    "        return xs\n",
    "    \n",
    "    def __feed_forward(self, xs, Wxh, Whh, Why, bh, by):\n",
    "        \n",
    "        hs, ys, ps = [], [], []\n",
    "        loss = 0\n",
    "        for t in range(len(xs)-1): # -1 because last word has no next word\n",
    "            # initial hidden state\n",
    "            if t == 0:\n",
    "                h_init = np.zeros((self.hidden_units,1))\n",
    "                hs.append(np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, h_init) + bh))\n",
    "            # hidden state > 0\n",
    "            else:\n",
    "                hs.append(np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh)) # previous hidden state influences actual hidden state\n",
    "            ys.append(np.dot(Why, hs[t]) + by) # unnormalized log probabilities for next chars\n",
    "            ps.append(np.exp(ys[t]) / np.sum(np.exp(ys[t]))) # probabilities for next chars\n",
    "            loss += -np.log(ps[t][xs[t+1],0]) # softmax \n",
    "        return loss, hs, ys, ps\n",
    "            \n",
    "        \n",
    "    def __backprop(self, xs, hs, ps, Wxh, Whh, Why, bh, by):\n",
    "        dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "        dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "        dhnext = np.zeros_like(hs[0])\n",
    "        for t in reversed(range(len(xs)-1)):\n",
    "            # compute how predicted scores should change to decrease loss: derieving gradient simplifies to:\n",
    "            dy = np.array(ps[t], copy=True)\n",
    "            dy[xs[t+1]] -= 1 # http://cs231n.github.io/neural-networks-case-study/#grad\n",
    "            dWhy += np.dot(dy, hs[t].T)\n",
    "            dby += dy\n",
    "            dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "            dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "            dbh += dhraw\n",
    "            dWxh += np.dot(dhraw, xs[t].T)\n",
    "            dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "            dhnext = np.dot(Whh.T, dhraw)\n",
    "            \n",
    "            for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "                np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "\n",
    "        return dWxh, dWhh, dWhy, dbh, dby\n",
    "    \n",
    "    def __param_update(self, )\n",
    "        \n",
    "    def train(self, echos):\n",
    "        \n",
    "        xs = self.__preprocess()\n",
    "        \n",
    "        # initially, set weight matrices randomly\n",
    "        Wxh = np.random.randn(self.hidden_units, self.feature_vec_size) * 0.01\n",
    "        Whh = np.random.randn(self.hidden_units, self.hidden_units) * 0.01\n",
    "        Why = np.random.randn(self.feature_vec_size, self.hidden_units) * 0.01\n",
    "        bh = np.zeros((self.hidden_units, 1)) # hidden bias\n",
    "        by = np.zeros((self.feature_vec_size, 1)) # output bias\n",
    "        \n",
    "        loss, hs, ys, ps = self.__feed_forward(xs, Wxh, Whh, Why, bh, by)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function clip in module numpy:\n",
      "\n",
      "clip(a, a_min, a_max, out=None, **kwargs)\n",
      "    Clip (limit) the values in an array.\n",
      "    \n",
      "    Given an interval, values outside the interval are clipped to\n",
      "    the interval edges.  For example, if an interval of ``[0, 1]``\n",
      "    is specified, values smaller than 0 become 0, and values larger\n",
      "    than 1 become 1.\n",
      "    \n",
      "    Equivalent to but faster than ``np.maximum(a_min, np.minimum(a, a_max))``.\n",
      "    No check is performed to ensure ``a_min < a_max``.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    a : array_like\n",
      "        Array containing elements to clip.\n",
      "    a_min : scalar or array_like or `None`\n",
      "        Minimum value. If `None`, clipping is not performed on lower\n",
      "        interval edge. Not more than one of `a_min` and `a_max` may be\n",
      "        `None`.\n",
      "    a_max : scalar or array_like or `None`\n",
      "        Maximum value. If `None`, clipping is not performed on upper\n",
      "        interval edge. Not more than one of `a_min` and `a_max` may be\n",
      "        `None`. If `a_min` or `a_max` are array_like, then the three\n",
      "        arrays will be broadcasted to match their shapes.\n",
      "    out : ndarray, optional\n",
      "        The results will be placed in this array. It may be the input\n",
      "        array for in-place clipping.  `out` must be of the right shape\n",
      "        to hold the output.  Its type is preserved.\n",
      "    **kwargs\n",
      "        For other keyword-only arguments, see the\n",
      "        :ref:`ufunc docs <ufuncs.kwargs>`.\n",
      "    \n",
      "        .. versionadded:: 1.17.0\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    clipped_array : ndarray\n",
      "        An array with the elements of `a`, but where values\n",
      "        < `a_min` are replaced with `a_min`, and those > `a_max`\n",
      "        with `a_max`.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    numpy.doc.ufuncs : Section \"Output arguments\"\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> a = np.arange(10)\n",
      "    >>> np.clip(a, 1, 8)\n",
      "    array([1, 1, 2, 3, 4, 5, 6, 7, 8, 8])\n",
      "    >>> a\n",
      "    array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "    >>> np.clip(a, 3, 6, out=a)\n",
      "    array([3, 3, 3, 3, 4, 5, 6, 6, 6, 6])\n",
      "    >>> a = np.arange(10)\n",
      "    >>> a\n",
      "    array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "    >>> np.clip(a, [3, 4, 1, 1, 1, 4, 4, 4, 4, 4], 8)\n",
      "    array([3, 4, 2, 3, 4, 5, 6, 7, 8, 8])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(np.clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.0099956 , 0.01000111, 0.01000344, 0.00999573, 0.01000225,\n",
       "         0.01000309, 0.00999874, 0.01000191, 0.00999946, 0.01000078,\n",
       "         0.00999762, 0.00999755, 0.01000365, 0.00999956, 0.01000453,\n",
       "         0.00999931, 0.01000181, 0.0099992 , 0.01000283, 0.01000259],\n",
       "        [0.01001087, 0.00999615, 0.00998991, 0.01001052, 0.00999309,\n",
       "         0.00999084, 0.01000246, 0.00999399, 0.01000053, 0.009997  ,\n",
       "         0.01000545, 0.01000565, 0.00998935, 0.01000028, 0.00998701,\n",
       "         0.01000095, 0.00999426, 0.01000124, 0.00999155, 0.00999219],\n",
       "        [0.00999402, 0.01000162, 0.01000484, 0.00999421, 0.0100032 ,\n",
       "         0.01000436, 0.00999836, 0.01000273, 0.00999935, 0.01000117,\n",
       "         0.00999682, 0.00999671, 0.01000513, 0.00999948, 0.01000634,\n",
       "         0.00999914, 0.01000259, 0.00999899, 0.01000399, 0.01000366],\n",
       "        [0.00999261, 0.01000208, 0.0100061 , 0.00999284, 0.01000405,\n",
       "         0.01000549, 0.00999801, 0.01000347, 0.00999925, 0.01000153,\n",
       "         0.00999609, 0.00999596, 0.01000646, 0.00999942, 0.01000797,\n",
       "         0.00999899, 0.01000329, 0.0099988 , 0.01000504, 0.01000462],\n",
       "        [0.0099992 , 0.00999994, 0.01000025, 0.00999921, 0.01000009,\n",
       "         0.0100002 , 0.00999962, 0.01000004, 0.00999972, 0.00999989,\n",
       "         0.00999947, 0.00999946, 0.01000028, 0.00999973, 0.01000039,\n",
       "         0.00999969, 0.01000003, 0.00999968, 0.01000017, 0.01000013]]),\n",
       " array([[0.01000117, 0.01000241, 0.00999782, 0.01000094, 0.00999683,\n",
       "         0.00999776, 0.00999831, 0.01000232, 0.01000416, 0.00999813,\n",
       "         0.00999914, 0.01000288, 0.00999992, 0.01000102, 0.01000299,\n",
       "         0.00999804, 0.00999757, 0.0100014 , 0.01000297, 0.00999409],\n",
       "        [0.00999662, 0.00999368, 0.0100061 , 0.00999724, 0.01000866,\n",
       "         0.01000623, 0.01000447, 0.00999398, 0.00998888, 0.01000509,\n",
       "         0.01000216, 0.00999218, 0.01000049, 0.00999729, 0.00999234,\n",
       "         0.01000523, 0.01000665, 0.00999623, 0.00999228, 0.01001603],\n",
       "        [0.01000153, 0.01000334, 0.00999706, 0.01000121, 0.00999568,\n",
       "         0.00999698, 0.00999765, 0.01000323, 0.01000573, 0.00999744,\n",
       "         0.00999878, 0.01000392, 0.00999996, 0.0100014 , 0.01000422,\n",
       "         0.00999729, 0.00999669, 0.01000192, 0.01000416, 0.0099919 ],\n",
       "        [0.01000252, 0.01000397, 0.00999578, 0.01000211, 0.00999424,\n",
       "         0.00999573, 0.00999722, 0.01000371, 0.01000719, 0.00999665,\n",
       "         0.00999879, 0.01000522, 0.00999937, 0.01000177, 0.01000455,\n",
       "         0.00999668, 0.00999556, 0.01000248, 0.01000473, 0.00998948],\n",
       "        [0.01000006, 0.01000036, 0.0099998 , 0.01000003, 0.00999964,\n",
       "         0.00999979, 0.00999975, 0.01000037, 0.01000056, 0.00999978,\n",
       "         0.00999984, 0.01000033, 0.01000009, 0.01000014, 0.01000053,\n",
       "         0.00999973, 0.00999973, 0.01000018, 0.01000048, 0.00999928]]),\n",
       " array([[0.0099949 , 0.01000134, 0.01000205, 0.00999433, 0.00999944,\n",
       "         0.00999998, 0.01000057, 0.00999821, 0.01000351, 0.01000229,\n",
       "         0.01000298, 0.00999764, 0.00999643, 0.01000181, 0.01000154,\n",
       "         0.01000022, 0.00999749, 0.00999793, 0.0100002 , 0.0099986 ],\n",
       "        [0.01001449, 0.00999728, 0.00999504, 0.01001601, 0.01000195,\n",
       "         0.01000057, 0.00999907, 0.01000563, 0.00999163, 0.00999444,\n",
       "         0.00999272, 0.01000725, 0.0100102 , 0.00999595, 0.00999672,\n",
       "         0.00999999, 0.01000724, 0.01000633, 0.01000031, 0.010004  ],\n",
       "        [0.0099929 , 0.01000182, 0.01000272, 0.0099921 , 0.00999911,\n",
       "         0.00999987, 0.01000068, 0.00999751, 0.01000484, 0.01000306,\n",
       "         0.01000401, 0.00999672, 0.00999502, 0.01000244, 0.01000213,\n",
       "         0.01000019, 0.00999642, 0.0099971 , 0.01000028, 0.0099979 ],\n",
       "        [0.00999092, 0.01000187, 0.01000368, 0.00998996, 0.00999932,\n",
       "         0.01000013, 0.01000105, 0.00999651, 0.01000537, 0.01000404,\n",
       "         0.01000508, 0.00999543, 0.00999376, 0.01000284, 0.01000217,\n",
       "         0.01000048, 0.00999586, 0.00999613, 0.00999987, 0.00999822],\n",
       "        [0.0099995 , 0.01000041, 0.01000039, 0.00999942, 0.01000002,\n",
       "         0.01000011, 0.0100002 , 0.00999999, 0.01000074, 0.01000043,\n",
       "         0.01000054, 0.00999992, 0.00999969, 0.01000043, 0.01000046,\n",
       "         0.01000015, 0.00999977, 0.00999992, 0.01000028, 0.00999984]]),\n",
       " array([[0.00999488, 0.01000133, 0.01000206, 0.00999431, 0.00999946,\n",
       "         0.01      , 0.01000057, 0.00999821, 0.01000348, 0.0100023 ,\n",
       "         0.01000297, 0.00999762, 0.00999645, 0.0100018 , 0.01000155,\n",
       "         0.01000022, 0.00999751, 0.00999793, 0.01000021, 0.00999864],\n",
       "        [0.01001402, 0.00999718, 0.00999534, 0.01001551, 0.01000212,\n",
       "         0.01000072, 0.00999922, 0.01000531, 0.00999156, 0.00999473,\n",
       "         0.00999298, 0.01000685, 0.01000993, 0.00999599, 0.0099966 ,\n",
       "         0.01000014, 0.01000721, 0.01000606, 0.0100001 , 0.01000431],\n",
       "        [0.00999277, 0.0100018 , 0.0100028 , 0.00999197, 0.00999916,\n",
       "         0.00999991, 0.01000071, 0.00999744, 0.01000481, 0.01000313,\n",
       "         0.01000407, 0.00999661, 0.00999496, 0.01000244, 0.01000211,\n",
       "         0.01000022, 0.00999643, 0.00999703, 0.01000023, 0.00999799],\n",
       "        [0.00999174, 0.01000203, 0.01000316, 0.00999083, 0.00999901,\n",
       "         0.00999987, 0.01000079, 0.00999705, 0.01000546, 0.01000354,\n",
       "         0.01000461, 0.00999611, 0.00999423, 0.01000276, 0.01000238,\n",
       "         0.01000023, 0.0099959 , 0.0099966 , 0.01000024, 0.00999768],\n",
       "        [0.00999935, 0.01000038, 0.01000048, 0.00999926, 0.01000007,\n",
       "         0.01000015, 0.01000025, 0.00999988, 0.01000072, 0.01000052,\n",
       "         0.01000063, 0.00999979, 0.00999959, 0.01000045, 0.01000041,\n",
       "         0.01000019, 0.00999976, 0.00999983, 0.0100002 , 0.00999993]]),\n",
       " array([[0.01000119, 0.01000239, 0.0099978 , 0.01000097, 0.00999684,\n",
       "         0.00999777, 0.0099983 , 0.01000233, 0.01000413, 0.00999811,\n",
       "         0.00999911, 0.01000289, 0.00999995, 0.010001  , 0.01000299,\n",
       "         0.00999803, 0.00999759, 0.01000141, 0.01000297, 0.0099941 ],\n",
       "        [0.00999653, 0.00999366, 0.01000596, 0.0099971 , 0.01000842,\n",
       "         0.01000596, 0.01000457, 0.00999367, 0.00998914, 0.01000516,\n",
       "         0.01000251, 0.00999215, 0.00999993, 0.00999741, 0.00999209,\n",
       "         0.01000526, 0.0100063 , 0.0099961 , 0.00999205, 0.0100157 ],\n",
       "        [0.01000152, 0.01000333, 0.00999702, 0.01000119, 0.00999562,\n",
       "         0.00999691, 0.00999766, 0.01000317, 0.01000578, 0.00999744,\n",
       "         0.00999884, 0.01000392, 0.00999984, 0.01000142, 0.01000415,\n",
       "         0.00999729, 0.00999661, 0.0100019 , 0.0100041 , 0.00999183],\n",
       "        [0.01000268, 0.01000398, 0.00999601, 0.01000236, 0.00999465,\n",
       "         0.00999618, 0.00999703, 0.01000424, 0.01000671, 0.0099965 ,\n",
       "         0.00999815, 0.01000526, 0.01000036, 0.01000153, 0.01000498,\n",
       "         0.00999662, 0.00999618, 0.0100027 , 0.01000511, 0.00999005],\n",
       "        [0.01000001, 0.01000035, 0.00999976, 0.00999997, 0.00999955,\n",
       "         0.00999969, 0.00999978, 0.01000026, 0.01000064, 0.0099998 ,\n",
       "         0.00999996, 0.01000032, 0.00999989, 0.01000018, 0.01000044,\n",
       "         0.00999974, 0.0099996 , 0.01000013, 0.0100004 , 0.00999916]]),\n",
       " array([[0.00999903, 0.00999719, 0.00999986, 0.00999675, 0.01000215,\n",
       "         0.00999517, 0.00999518, 0.00999972, 0.0100008 , 0.01000036,\n",
       "         0.00999499, 0.01000298, 0.0100006 , 0.01000081, 0.0100013 ,\n",
       "         0.01000211, 0.01000053, 0.0100053 , 0.01000275, 0.0100009 ],\n",
       "        [0.01000287, 0.01000778, 0.01000032, 0.01000897, 0.00999418,\n",
       "         0.01001289, 0.0100129 , 0.01000106, 0.0099982 , 0.00999901,\n",
       "         0.01001341, 0.00999241, 0.00999858, 0.009998  , 0.00999683,\n",
       "         0.00999437, 0.0099986 , 0.0099861 , 0.00999297, 0.00999735],\n",
       "        [0.00999865, 0.00999617, 0.00999976, 0.00999549, 0.01000288,\n",
       "         0.00999328, 0.00999331, 0.00999963, 0.01000119, 0.01000045,\n",
       "         0.00999308, 0.01000413, 0.01000079, 0.01000114, 0.01000185,\n",
       "         0.01000285, 0.01000065, 0.01000731, 0.01000384, 0.0100011 ],\n",
       "        [0.00999813, 0.00999486, 0.01000002, 0.00999423, 0.01000406,\n",
       "         0.00999195, 0.0099919 , 0.00999919, 0.01000086, 0.01000084,\n",
       "         0.0099915 , 0.01000472, 0.01000098, 0.01000126, 0.01000185,\n",
       "         0.01000384, 0.01000116, 0.0100089 , 0.01000434, 0.01000225],\n",
       "        [0.00999992, 0.00999971, 0.00999996, 0.0099996 , 0.01000024,\n",
       "         0.00999932, 0.00999934, 0.01000005, 0.01000025, 0.01000003,\n",
       "         0.00999934, 0.01000049, 0.0100001 , 0.01000017, 0.01000028,\n",
       "         0.01000026, 0.01000003, 0.01000077, 0.01000047, 0.01      ]]),\n",
       " array([[0.009999  , 0.0099972 , 0.00999988, 0.00999672, 0.01000215,\n",
       "         0.00999519, 0.0099952 , 0.0099997 , 0.01000081, 0.01000037,\n",
       "         0.00999502, 0.01000295, 0.01000058, 0.01000082, 0.0100013 ,\n",
       "         0.01000211, 0.01000052, 0.01000527, 0.01000274, 0.01000091],\n",
       "        [0.01000271, 0.01000739, 0.01000046, 0.01000866, 0.00999457,\n",
       "         0.0100127 , 0.01001267, 0.01000087, 0.00999796, 0.00999917,\n",
       "         0.0100131 , 0.00999241, 0.00999863, 0.00999798, 0.0099967 ,\n",
       "         0.00999466, 0.00999881, 0.00998638, 0.00999295, 0.00999784],\n",
       "        [0.00999859, 0.00999608, 0.00999981, 0.0099954 , 0.01000298,\n",
       "         0.00999325, 0.00999326, 0.00999958, 0.01000114, 0.0100005 ,\n",
       "         0.00999303, 0.01000412, 0.01000079, 0.01000113, 0.01000181,\n",
       "         0.01000293, 0.0100007 , 0.01000737, 0.01000383, 0.01000123],\n",
       "        [0.0099984 , 0.00999553, 0.00999977, 0.00999476, 0.01000339,\n",
       "         0.00999229, 0.00999231, 0.00999952, 0.0100013 , 0.01000056,\n",
       "         0.00999204, 0.0100047 , 0.0100009 , 0.01000129, 0.01000207,\n",
       "         0.01000333, 0.01000079, 0.0100084 , 0.01000437, 0.01000139],\n",
       "        [0.00999987, 0.00999959, 0.01      , 0.00999951, 0.01000036,\n",
       "         0.00999926, 0.00999926, 0.00999999, 0.01000017, 0.01000008,\n",
       "         0.00999923, 0.0100005 , 0.01000012, 0.01000016, 0.01000024,\n",
       "         0.01000036, 0.01000011, 0.01000087, 0.01000047, 0.01000016]]),\n",
       " array([[0.01000116, 0.01000243, 0.00999782, 0.01000095, 0.00999682,\n",
       "         0.0099978 , 0.00999834, 0.01000232, 0.01000415, 0.00999812,\n",
       "         0.00999918, 0.01000285, 0.00999992, 0.01000101, 0.01000299,\n",
       "         0.00999802, 0.00999757, 0.01000136, 0.01000296, 0.00999409],\n",
       "        [0.00999685, 0.00999337, 0.01000582, 0.00999729, 0.01000863,\n",
       "         0.01000562, 0.01000418, 0.00999379, 0.00998896, 0.01000504,\n",
       "         0.01000194, 0.00999256, 0.01000025, 0.00999735, 0.00999208,\n",
       "         0.01000541, 0.01000653, 0.00999665, 0.00999225, 0.01001588],\n",
       "        [0.01000158, 0.01000328, 0.00999699, 0.01000123, 0.00999566,\n",
       "         0.00999685, 0.00999759, 0.01000319, 0.01000575, 0.00999742,\n",
       "         0.00999874, 0.010004  , 0.0099999 , 0.01000141, 0.01000416,\n",
       "         0.00999732, 0.00999666, 0.010002  , 0.01000414, 0.00999186],\n",
       "        [0.01000212, 0.01000451, 0.00999628, 0.01000202, 0.00999428,\n",
       "         0.00999679, 0.00999772, 0.01000403, 0.01000705, 0.00999674,\n",
       "         0.00999917, 0.01000455, 0.0099998 , 0.01000164, 0.01000499,\n",
       "         0.00999635, 0.00999576, 0.01000172, 0.01000476, 0.00998973],\n",
       "        [0.01000013, 0.01000025, 0.00999971, 0.01000004, 0.00999963,\n",
       "         0.00999957, 0.00999965, 0.0100003 , 0.01000058, 0.00999976,\n",
       "         0.00999975, 0.01000046, 0.01000001, 0.01000016, 0.01000044,\n",
       "         0.00999979, 0.00999969, 0.01000033, 0.01000047, 0.00999923]])]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'Jane saw Marry. Marry saw James. James saw Doug.'\n",
    "model = Vanilla_rnn(1e-1, 20, 3, sentence)\n",
    "model.train(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function copy in module numpy:\n",
      "\n",
      "copy(a, order='K')\n",
      "    Return an array copy of the given object.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    a : array_like\n",
      "        Input data.\n",
      "    order : {'C', 'F', 'A', 'K'}, optional\n",
      "        Controls the memory layout of the copy. 'C' means C-order,\n",
      "        'F' means F-order, 'A' means 'F' if `a` is Fortran contiguous,\n",
      "        'C' otherwise. 'K' means match the layout of `a` as closely\n",
      "        as possible. (Note that this function and :meth:`ndarray.copy` are very\n",
      "        similar, but have different default values for their order=\n",
      "        arguments.)\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    arr : ndarray\n",
      "        Array interpretation of `a`.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    This is equivalent to:\n",
      "    \n",
      "    >>> np.array(a, copy=True)  #doctest: +SKIP\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    Create an array x, with a reference y and a copy z:\n",
      "    \n",
      "    >>> x = np.array([1, 2, 3])\n",
      "    >>> y = x\n",
      "    >>> z = np.copy(x)\n",
      "    \n",
      "    Note that, when we modify x, y changes, but not z:\n",
      "    \n",
      "    >>> x[0] = 10\n",
      "    >>> x[0] == y[0]\n",
      "    True\n",
      "    >>> x[0] == z[0]\n",
      "    False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(np.copy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
